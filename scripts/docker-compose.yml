services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11435:11434" # Porta no host (11435) -> porta interna do ollama (11434, padrão)
    volumes:
      - ollama_data:/root/.ollama # Persistência dos modelos
    entrypoint:
      > # Comandos para iniciar o servidor do Ollama e baixar o modelo de Embedding
      /bin/sh -c "
      ollama serve &
      sleep 5 &&
      ollama pull bge-m3:latest &&
      tail -f /dev/null
      "
  api:
    build:
      context: ../src # Configura o contexto, no caso, volta pra pasta raiz, e entra na "/src"
      dockerfile: api/Dockerfile # A partir do contexto, indica o caminho até o Dockerfile
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY} # Passa a variável de ambiente do host para o container
      - OLLAMA_HOST=http://ollama:11434 # API usa o serviço interno do ollama
    ports:
      - "80:80"
    depends_on:
      - ollama

  front:
    build:
      context: ../src/front
      dockerfile: Dockerfile
    environment:
      - API_URL=http://api:80
    ports:
      - "8501:8501"
    depends_on:
      - api

volumes:
  ollama_data:
